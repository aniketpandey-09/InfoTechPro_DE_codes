{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03214786-457c-427c-ac74-c2df034c1554",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"ETL_FactTable\").getOrCreate()\n",
    "\n",
    "# Load SalesOrderHeader and SalesOrderDetail from local paths in DBFS\n",
    "sales_order_header_df = spark.read.csv(\"dbfs:/FileStore/tables/production/salesorderheader.csv\", header=True, inferSchema=True) \\\n",
    "    .filter(col(\"OnlineOrderFlag\") == 1) \\\n",
    "    .select(\n",
    "        col(\"SalesOrderID\"),\n",
    "        col(\"SalesOrderNumber\"),\n",
    "        to_date(col(\"OrderDate\")).alias(\"OrderDate\"),\n",
    "        col(\"CustomerID\"),\n",
    "        col(\"TerritoryID\"),\n",
    "        col(\"TaxAmt\"),\n",
    "        col(\"Freight\")\n",
    "    )\n",
    "\n",
    "sales_order_detail_df = spark.read.csv(\"dbfs:/FileStore/tables/production/salesorderdetails.csv\", header=True, inferSchema=True)\n",
    "product_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/tables/production/Production_product.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc42430d-bc79-4741-b728-cbb23f8789f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sort data by SalesOrderID\n",
    "sales_order_header_sorted = sales_order_header.orderBy(\"SalesOrderID\")\n",
    "sales_order_detail_sorted = sales_order_detail.orderBy(\"SalesOrderID\")\n",
    "\n",
    "# Merge Join based on SalesOrderID\n",
    "merged_data = sales_order_header_sorted.join(\n",
    "    sales_order_detail_sorted,\n",
    "    \"SalesOrderID\",\n",
    "    \"inner\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3fe9a5a-31cf-4206-a728-3ff42e1c82fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2715838316518987>:6\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m product_dim \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mcsv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/FileStore/tables/dimension/dim_product_sale\u001B[39m\u001B[38;5;124m\"\u001B[39m, header\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, inferSchema\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# 9.3 Lookup Transformation - Add product_sk from product_df based on ProductID\u001B[39;00m\n",
       "\u001B[1;32m      5\u001B[0m merged_data_with_product_sk \u001B[38;5;241m=\u001B[39m merged_data\u001B[38;5;241m.\u001B[39mjoin(\n",
       "\u001B[0;32m----> 6\u001B[0m     product_df\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproductID\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproduct_sk\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n",
       "\u001B[1;32m      7\u001B[0m     merged_data\u001B[38;5;241m.\u001B[39mProductID \u001B[38;5;241m==\u001B[39m product_df\u001B[38;5;241m.\u001B[39mproduct_id,\n",
       "\u001B[1;32m      8\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mleft\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m      9\u001B[0m )\u001B[38;5;241m.\u001B[39mdrop(product_df\u001B[38;5;241m.\u001B[39mproduct_id)\n",
       "\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# 9.4 DRC - Replace nulls\u001B[39;00m\n",
       "\u001B[1;32m     12\u001B[0m merged_data_validated \u001B[38;5;241m=\u001B[39m merged_data_with_product_sk \\\n",
       "\u001B[1;32m     13\u001B[0m     \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTaxAmt\u001B[39m\u001B[38;5;124m\"\u001B[39m, coalesce(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTaxAmt\u001B[39m\u001B[38;5;124m\"\u001B[39m), lit(\u001B[38;5;241m0\u001B[39m))) \\\n",
       "\u001B[1;32m     14\u001B[0m     \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFreight\u001B[39m\u001B[38;5;124m\"\u001B[39m, coalesce(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFreight\u001B[39m\u001B[38;5;124m\"\u001B[39m), lit(\u001B[38;5;241m0\u001B[39m))) \\\n",
       "\u001B[1;32m     15\u001B[0m     \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproduct_sk\u001B[39m\u001B[38;5;124m\"\u001B[39m, coalesce(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproduct_sk\u001B[39m\u001B[38;5;124m\"\u001B[39m), lit(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)))  \u001B[38;5;66;03m# Use -1 for missing product_sk\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3023\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n",
       "\u001B[1;32m   2978\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n",
       "\u001B[1;32m   2979\u001B[0m     \u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n",
       "\u001B[1;32m   2980\u001B[0m \n",
       "\u001B[1;32m   2981\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   3021\u001B[0m \u001B[38;5;124;03m    +-----+---+\u001B[39;00m\n",
       "\u001B[1;32m   3022\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 3023\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jcols\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   3024\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `product_sk` cannot be resolved. Did you mean one of the following? [`ProductID`, `ProductLine`, `ProductNumber`, `ProductModelID`, `rowguid`].;\n",
       "'Project [productID#5138, 'product_sk]\n",
       "+- Relation [ProductID#5138,Name#5139,ProductNumber#5140,MakeFlag#5141,FinishedGoodsFlag#5142,Color#5143,SafetyStockLevel#5144,ReorderPoint#5145,StandardCost#5146,ListPrice#5147,Size#5148,SizeUnitMeasureCode#5149,WeightUnitMeasureCode#5150,Weight#5151,DaysToManufacture#5152,ProductLine#5153,Class#5154,Style#5155,ProductSubcategoryID#5156,ProductModelID#5157,SellStartDate#5158,SellEndDate#5159,DiscontinuedDate#5160,rowguid#5161,ModifiedDate#5162] csv\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-2715838316518987>:6\u001B[0m\n\u001B[1;32m      2\u001B[0m product_dim \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mcsv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/FileStore/tables/dimension/dim_product_sale\u001B[39m\u001B[38;5;124m\"\u001B[39m, header\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, inferSchema\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# 9.3 Lookup Transformation - Add product_sk from product_df based on ProductID\u001B[39;00m\n\u001B[1;32m      5\u001B[0m merged_data_with_product_sk \u001B[38;5;241m=\u001B[39m merged_data\u001B[38;5;241m.\u001B[39mjoin(\n\u001B[0;32m----> 6\u001B[0m     product_df\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproductID\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproduct_sk\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m      7\u001B[0m     merged_data\u001B[38;5;241m.\u001B[39mProductID \u001B[38;5;241m==\u001B[39m product_df\u001B[38;5;241m.\u001B[39mproduct_id,\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mleft\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      9\u001B[0m )\u001B[38;5;241m.\u001B[39mdrop(product_df\u001B[38;5;241m.\u001B[39mproduct_id)\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# 9.4 DRC - Replace nulls\u001B[39;00m\n\u001B[1;32m     12\u001B[0m merged_data_validated \u001B[38;5;241m=\u001B[39m merged_data_with_product_sk \\\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTaxAmt\u001B[39m\u001B[38;5;124m\"\u001B[39m, coalesce(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTaxAmt\u001B[39m\u001B[38;5;124m\"\u001B[39m), lit(\u001B[38;5;241m0\u001B[39m))) \\\n\u001B[1;32m     14\u001B[0m     \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFreight\u001B[39m\u001B[38;5;124m\"\u001B[39m, coalesce(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFreight\u001B[39m\u001B[38;5;124m\"\u001B[39m), lit(\u001B[38;5;241m0\u001B[39m))) \\\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproduct_sk\u001B[39m\u001B[38;5;124m\"\u001B[39m, coalesce(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproduct_sk\u001B[39m\u001B[38;5;124m\"\u001B[39m), lit(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)))  \u001B[38;5;66;03m# Use -1 for missing product_sk\u001B[39;00m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3023\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m   2978\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   2979\u001B[0m     \u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m   2980\u001B[0m \n\u001B[1;32m   2981\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3021\u001B[0m \u001B[38;5;124;03m    +-----+---+\u001B[39;00m\n\u001B[1;32m   3022\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 3023\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jcols\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3024\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `product_sk` cannot be resolved. Did you mean one of the following? [`ProductID`, `ProductLine`, `ProductNumber`, `ProductModelID`, `rowguid`].;\n'Project [productID#5138, 'product_sk]\n+- Relation [ProductID#5138,Name#5139,ProductNumber#5140,MakeFlag#5141,FinishedGoodsFlag#5142,Color#5143,SafetyStockLevel#5144,ReorderPoint#5145,StandardCost#5146,ListPrice#5147,Size#5148,SizeUnitMeasureCode#5149,WeightUnitMeasureCode#5150,Weight#5151,DaysToManufacture#5152,ProductLine#5153,Class#5154,Style#5155,ProductSubcategoryID#5156,ProductModelID#5157,SellStartDate#5158,SellEndDate#5159,DiscontinuedDate#5160,rowguid#5161,ModifiedDate#5162] csv\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `product_sk` cannot be resolved. Did you mean one of the following? [`ProductID`, `ProductLine`, `ProductNumber`, `ProductModelID`, `rowguid`].;\n'Project [productID#5138, 'product_sk]\n+- Relation [ProductID#5138,Name#5139,ProductNumber#5140,MakeFlag#5141,FinishedGoodsFlag#5142,Color#5143,SafetyStockLevel#5144,ReorderPoint#5145,StandardCost#5146,ListPrice#5147,Size#5148,SizeUnitMeasureCode#5149,WeightUnitMeasureCode#5150,Weight#5151,DaysToManufacture#5152,ProductLine#5153,Class#5154,Style#5155,ProductSubcategoryID#5156,ProductModelID#5157,SellStartDate#5158,SellEndDate#5159,DiscontinuedDate#5160,rowguid#5161,ModifiedDate#5162] csv\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load product dimension data\n",
    "product_dim = spark.read.csv(\"dbfs:/FileStore/tables/dimension/dim_product_sale\", header=True, inferSchema=True)\n",
    "\n",
    "# 9.3 Lookup Transformation - Add product_sk from product_df based on ProductID\n",
    "merged_data_with_product_sk = merged_data.join(\n",
    "    product_df.select(\"productID\", \"product_sk\"),\n",
    "    merged_data.ProductID == product_df.product_id,\n",
    "    \"left\"\n",
    ").drop(product_df.product_id)\n",
    "\n",
    "# 9.4 DRC - Replace nulls\n",
    "merged_data_validated = merged_data_with_product_sk \\\n",
    "    .withColumn(\"TaxAmt\", coalesce(col(\"TaxAmt\"), lit(0))) \\\n",
    "    .withColumn(\"Freight\", coalesce(col(\"Freight\"), lit(0))) \\\n",
    "    .withColumn(\"product_sk\", coalesce(col(\"product_sk\"), lit(-1)))  # Use -1 for missing product_sk\n",
    "\n",
    "# 9.5 DRC - Add additional fields like extended_sales and extended_cost\n",
    "merged_data_final = merged_data_validated \\\n",
    "    .withColumn(\"extended_sales\", col(\"OrderQty\") * col(\"UnitPrice\")) \\\n",
    "    .withColumn(\"extended_cost\", col(\"OrderQty\") * col(\"standard_cost\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "128353d5-dc38-4674-8a28-cdc3fb287814",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2715838316518990>:38\u001B[0m\n",
       "\u001B[1;32m     30\u001B[0m merged_data \u001B[38;5;241m=\u001B[39m sales_order_header_sorted\u001B[38;5;241m.\u001B[39mjoin(\n",
       "\u001B[1;32m     31\u001B[0m     sales_order_detail_sorted,\n",
       "\u001B[1;32m     32\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSalesOrderID\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m     33\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minner\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     34\u001B[0m )\n",
       "\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# 9.3 Lookup Transformation - Add product_sk from product_df based on ProductID\u001B[39;00m\n",
       "\u001B[1;32m     37\u001B[0m merged_data_with_product_sk \u001B[38;5;241m=\u001B[39m merged_data\u001B[38;5;241m.\u001B[39mjoin(\n",
       "\u001B[0;32m---> 38\u001B[0m     product_df\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproduct_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproduct_sk\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n",
       "\u001B[1;32m     39\u001B[0m     merged_data\u001B[38;5;241m.\u001B[39mProductID \u001B[38;5;241m==\u001B[39m product_df\u001B[38;5;241m.\u001B[39mproduct_id,\n",
       "\u001B[1;32m     40\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mleft\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     41\u001B[0m )\u001B[38;5;241m.\u001B[39mdrop(product_df\u001B[38;5;241m.\u001B[39mproduct_id)\n",
       "\u001B[1;32m     43\u001B[0m \u001B[38;5;66;03m# 9.4 DRC - Replace nulls\u001B[39;00m\n",
       "\u001B[1;32m     44\u001B[0m merged_data_validated \u001B[38;5;241m=\u001B[39m merged_data_with_product_sk \\\n",
       "\u001B[1;32m     45\u001B[0m     \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTaxAmt\u001B[39m\u001B[38;5;124m\"\u001B[39m, coalesce(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTaxAmt\u001B[39m\u001B[38;5;124m\"\u001B[39m), lit(\u001B[38;5;241m0\u001B[39m))) \\\n",
       "\u001B[1;32m     46\u001B[0m     \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFreight\u001B[39m\u001B[38;5;124m\"\u001B[39m, coalesce(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFreight\u001B[39m\u001B[38;5;124m\"\u001B[39m), lit(\u001B[38;5;241m0\u001B[39m))) \\\n",
       "\u001B[1;32m     47\u001B[0m     \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproduct_sk\u001B[39m\u001B[38;5;124m\"\u001B[39m, coalesce(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproduct_sk\u001B[39m\u001B[38;5;124m\"\u001B[39m), lit(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)))  \u001B[38;5;66;03m# Use -1 for missing product_sk\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3023\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n",
       "\u001B[1;32m   2978\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n",
       "\u001B[1;32m   2979\u001B[0m     \u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n",
       "\u001B[1;32m   2980\u001B[0m \n",
       "\u001B[1;32m   2981\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   3021\u001B[0m \u001B[38;5;124;03m    +-----+---+\u001B[39;00m\n",
       "\u001B[1;32m   3022\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 3023\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jcols\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   3024\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `product_id` cannot be resolved. Did you mean one of the following? [`ProductID`, `ProductLine`, `rowguid`, `ProductModelID`, `ProductNumber`].;\n",
       "'Project ['product_id, 'product_sk]\n",
       "+- Relation [ProductID#5368,Name#5369,ProductNumber#5370,MakeFlag#5371,FinishedGoodsFlag#5372,Color#5373,SafetyStockLevel#5374,ReorderPoint#5375,StandardCost#5376,ListPrice#5377,Size#5378,SizeUnitMeasureCode#5379,WeightUnitMeasureCode#5380,Weight#5381,DaysToManufacture#5382,ProductLine#5383,Class#5384,Style#5385,ProductSubcategoryID#5386,ProductModelID#5387,SellStartDate#5388,SellEndDate#5389,DiscontinuedDate#5390,rowguid#5391,ModifiedDate#5392] csv\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-2715838316518990>:38\u001B[0m\n\u001B[1;32m     30\u001B[0m merged_data \u001B[38;5;241m=\u001B[39m sales_order_header_sorted\u001B[38;5;241m.\u001B[39mjoin(\n\u001B[1;32m     31\u001B[0m     sales_order_detail_sorted,\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSalesOrderID\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     33\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minner\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     34\u001B[0m )\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# 9.3 Lookup Transformation - Add product_sk from product_df based on ProductID\u001B[39;00m\n\u001B[1;32m     37\u001B[0m merged_data_with_product_sk \u001B[38;5;241m=\u001B[39m merged_data\u001B[38;5;241m.\u001B[39mjoin(\n\u001B[0;32m---> 38\u001B[0m     product_df\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproduct_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproduct_sk\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m     39\u001B[0m     merged_data\u001B[38;5;241m.\u001B[39mProductID \u001B[38;5;241m==\u001B[39m product_df\u001B[38;5;241m.\u001B[39mproduct_id,\n\u001B[1;32m     40\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mleft\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     41\u001B[0m )\u001B[38;5;241m.\u001B[39mdrop(product_df\u001B[38;5;241m.\u001B[39mproduct_id)\n\u001B[1;32m     43\u001B[0m \u001B[38;5;66;03m# 9.4 DRC - Replace nulls\u001B[39;00m\n\u001B[1;32m     44\u001B[0m merged_data_validated \u001B[38;5;241m=\u001B[39m merged_data_with_product_sk \\\n\u001B[1;32m     45\u001B[0m     \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTaxAmt\u001B[39m\u001B[38;5;124m\"\u001B[39m, coalesce(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTaxAmt\u001B[39m\u001B[38;5;124m\"\u001B[39m), lit(\u001B[38;5;241m0\u001B[39m))) \\\n\u001B[1;32m     46\u001B[0m     \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFreight\u001B[39m\u001B[38;5;124m\"\u001B[39m, coalesce(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFreight\u001B[39m\u001B[38;5;124m\"\u001B[39m), lit(\u001B[38;5;241m0\u001B[39m))) \\\n\u001B[1;32m     47\u001B[0m     \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproduct_sk\u001B[39m\u001B[38;5;124m\"\u001B[39m, coalesce(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproduct_sk\u001B[39m\u001B[38;5;124m\"\u001B[39m), lit(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)))  \u001B[38;5;66;03m# Use -1 for missing product_sk\u001B[39;00m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3023\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m   2978\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   2979\u001B[0m     \u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m   2980\u001B[0m \n\u001B[1;32m   2981\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3021\u001B[0m \u001B[38;5;124;03m    +-----+---+\u001B[39;00m\n\u001B[1;32m   3022\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 3023\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jcols\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3024\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `product_id` cannot be resolved. Did you mean one of the following? [`ProductID`, `ProductLine`, `rowguid`, `ProductModelID`, `ProductNumber`].;\n'Project ['product_id, 'product_sk]\n+- Relation [ProductID#5368,Name#5369,ProductNumber#5370,MakeFlag#5371,FinishedGoodsFlag#5372,Color#5373,SafetyStockLevel#5374,ReorderPoint#5375,StandardCost#5376,ListPrice#5377,Size#5378,SizeUnitMeasureCode#5379,WeightUnitMeasureCode#5380,Weight#5381,DaysToManufacture#5382,ProductLine#5383,Class#5384,Style#5385,ProductSubcategoryID#5386,ProductModelID#5387,SellStartDate#5388,SellEndDate#5389,DiscontinuedDate#5390,rowguid#5391,ModifiedDate#5392] csv\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `product_id` cannot be resolved. Did you mean one of the following? [`ProductID`, `ProductLine`, `rowguid`, `ProductModelID`, `ProductNumber`].;\n'Project ['product_id, 'product_sk]\n+- Relation [ProductID#5368,Name#5369,ProductNumber#5370,MakeFlag#5371,FinishedGoodsFlag#5372,Color#5373,SafetyStockLevel#5374,ReorderPoint#5375,StandardCost#5376,ListPrice#5377,Size#5378,SizeUnitMeasureCode#5379,WeightUnitMeasureCode#5380,Weight#5381,DaysToManufacture#5382,ProductLine#5383,Class#5384,Style#5385,ProductSubcategoryID#5386,ProductModelID#5387,SellStartDate#5388,SellEndDate#5389,DiscontinuedDate#5390,rowguid#5391,ModifiedDate#5392] csv\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, coalesce, lit, to_date, expr\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"FactTableETL\").getOrCreate()\n",
    "\n",
    "# Load the data into DataFrames\n",
    "#product_df = spark.read.option(\"header\", \"true\").csv(\"/dbfs/FileStore/tables/dim_product_table.csv\", inferSchema=True)\n",
    "#sales_order_header_df = spark.read.option(\"header\", \"true\").csv(\"/dbfs/FileStore/tables/SalesOrderHeader.csv\", inferSchema=True)\n",
    "#sales_order_detail_df = spark.read.option(\"header\", \"true\").csv(\"/dbfs/FileStore/tables/SalesOrderDetail.csv\", inferSchema=True)\n",
    "\n",
    "# 9.1 Extract Phase - Filter online sales from SalesOrderHeader\n",
    "sales_order_header_filtered = sales_order_header_df.filter(col(\"OnlineOrderFlag\") == 1) \\\n",
    "    .select(\n",
    "        col(\"SalesOrderID\"),\n",
    "        col(\"SalesOrderNumber\"),\n",
    "        to_date(col(\"OrderDate\")).alias(\"OrderDate\"),\n",
    "        col(\"CustomerID\"),\n",
    "        col(\"TerritoryID\"),\n",
    "        col(\"TaxAmt\"),\n",
    "        col(\"Freight\")\n",
    "    )\n",
    "\n",
    "# 9.2 Transform Phase - Sort and Merge Join on SalesOrderID\n",
    "# Sort both dataframes by SalesOrderID\n",
    "sales_order_header_sorted = sales_order_header_filtered.orderBy(\"SalesOrderID\")\n",
    "sales_order_detail_sorted = sales_order_detail_df.orderBy(\"SalesOrderID\")\n",
    "\n",
    "# Merge join on SalesOrderID\n",
    "merged_data = sales_order_header_sorted.join(\n",
    "    sales_order_detail_sorted,\n",
    "    \"SalesOrderID\",\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "# 9.3 Lookup Transformation - Add product_sk from product_df based on ProductID\n",
    "merged_data_with_product_sk = merged_data.join(\n",
    "    product_df.select(\"product_id\", \"product_sk\"),\n",
    "    merged_data.ProductID == product_df.product_id,\n",
    "    \"left\"\n",
    ").drop(product_df.product_id)\n",
    "\n",
    "# 9.4 DRC - Replace nulls\n",
    "merged_data_validated = merged_data_with_product_sk \\\n",
    "    .withColumn(\"TaxAmt\", coalesce(col(\"TaxAmt\"), lit(0))) \\\n",
    "    .withColumn(\"Freight\", coalesce(col(\"Freight\"), lit(0))) \\\n",
    "    .withColumn(\"product_sk\", coalesce(col(\"product_sk\"), lit(-1)))  # Use -1 for missing product_sk\n",
    "\n",
    "# 9.5 DRC - Add additional fields like extended_sales and extended_cost\n",
    "merged_data_final = merged_data_validated \\\n",
    "    .withColumn(\"extended_sales\", col(\"OrderQty\") * col(\"UnitPrice\")) \\\n",
    "    .withColumn(\"extended_cost\", col(\"OrderQty\") * col(\"standard_cost\"))\n",
    "\n",
    "# 9.6 Load Phase - Select columns for the fact table\n",
    "fact_table = merged_data_final.select(\n",
    "    col(\"product_sk\"),\n",
    "    col(\"SalesOrderID\").alias(\"sales_order_id\"),\n",
    "    col(\"SalesOrderDetailID\").alias(\"line_number\"),\n",
    "    col(\"OrderQty\").alias(\"quantity\"),\n",
    "    col(\"UnitPrice\").alias(\"unit_price\"),\n",
    "    col(\"standard_cost\").alias(\"unit_cost\"),\n",
    "    col(\"TaxAmt\").alias(\"tax_amount\"),\n",
    "    col(\"Freight\").alias(\"freight\"),\n",
    "    col(\"extended_sales\"),\n",
    "    col(\"OrderDate\").alias(\"created_at\"),\n",
    "    col(\"extended_cost\")\n",
    ")\n",
    "\n",
    "# Writing to a Fact Table - Replace placeholders with appropriate details\n",
    "fact_table.write.format(\"jdbc\").options(\n",
    "    url=\"jdbc:sqlserver://<server>;database=<database>\",\n",
    "    dbtable=\"FactSalesTable\",\n",
    "    user=\"<user>\",\n",
    "    password=\"<password>\"\n",
    ").mode(\"append\").save()\n",
    "\n",
    "# Check the result\n",
    "fact_table.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "408c31d0-23a5-483c-8b92-b93595b4d540",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2715838316518993>:10\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m merged_data \u001B[38;5;241m=\u001B[39m sales_order_header_sorted\u001B[38;5;241m.\u001B[39mjoin(\n",
       "\u001B[1;32m      3\u001B[0m     sales_order_detail_sorted,\n",
       "\u001B[1;32m      4\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSalesOrderID\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m      5\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minner\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m      6\u001B[0m )\n",
       "\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# 9.3 Lookup Transformation - Add product_sk from product_df based on ProductID\u001B[39;00m\n",
       "\u001B[1;32m      9\u001B[0m merged_data_with_product_sk \u001B[38;5;241m=\u001B[39m merged_data\u001B[38;5;241m.\u001B[39mjoin(\n",
       "\u001B[0;32m---> 10\u001B[0m     product_df\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproduct_id\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n",
       "\u001B[1;32m     11\u001B[0m     merged_data\u001B[38;5;241m.\u001B[39mProduct_id \u001B[38;5;241m==\u001B[39m product_df\u001B[38;5;241m.\u001B[39mproduct_id,\n",
       "\u001B[1;32m     12\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mleft\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     13\u001B[0m )\u001B[38;5;241m.\u001B[39mdrop(product_df\u001B[38;5;241m.\u001B[39mproduct_id)\n",
       "\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# 9.4 DRC - Replace nulls\u001B[39;00m\n",
       "\u001B[1;32m     16\u001B[0m merged_data_validated \u001B[38;5;241m=\u001B[39m merged_data_with_product_sk \\\n",
       "\u001B[1;32m     17\u001B[0m     \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTaxAmt\u001B[39m\u001B[38;5;124m\"\u001B[39m, coalesce(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTaxAmt\u001B[39m\u001B[38;5;124m\"\u001B[39m), lit(\u001B[38;5;241m0\u001B[39m))) \\\n",
       "\u001B[1;32m     18\u001B[0m     \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFreight\u001B[39m\u001B[38;5;124m\"\u001B[39m, coalesce(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFreight\u001B[39m\u001B[38;5;124m\"\u001B[39m), lit(\u001B[38;5;241m0\u001B[39m))) \\\n",
       "\u001B[1;32m     19\u001B[0m     \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproduct_sk\u001B[39m\u001B[38;5;124m\"\u001B[39m, coalesce(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproduct_sk\u001B[39m\u001B[38;5;124m\"\u001B[39m), lit(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)))\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3023\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n",
       "\u001B[1;32m   2978\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n",
       "\u001B[1;32m   2979\u001B[0m     \u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n",
       "\u001B[1;32m   2980\u001B[0m \n",
       "\u001B[1;32m   2981\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   3021\u001B[0m \u001B[38;5;124;03m    +-----+---+\u001B[39;00m\n",
       "\u001B[1;32m   3022\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 3023\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jcols\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   3024\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `product_id` cannot be resolved. Did you mean one of the following? [`ProductID`, `ProductLine`, `rowguid`, `ProductModelID`, `ProductNumber`].;\n",
       "'Project ['product_id]\n",
       "+- Relation [ProductID#5368,Name#5369,ProductNumber#5370,MakeFlag#5371,FinishedGoodsFlag#5372,Color#5373,SafetyStockLevel#5374,ReorderPoint#5375,StandardCost#5376,ListPrice#5377,Size#5378,SizeUnitMeasureCode#5379,WeightUnitMeasureCode#5380,Weight#5381,DaysToManufacture#5382,ProductLine#5383,Class#5384,Style#5385,ProductSubcategoryID#5386,ProductModelID#5387,SellStartDate#5388,SellEndDate#5389,DiscontinuedDate#5390,rowguid#5391,ModifiedDate#5392] csv\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-2715838316518993>:10\u001B[0m\n\u001B[1;32m      2\u001B[0m merged_data \u001B[38;5;241m=\u001B[39m sales_order_header_sorted\u001B[38;5;241m.\u001B[39mjoin(\n\u001B[1;32m      3\u001B[0m     sales_order_detail_sorted,\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSalesOrderID\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minner\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      6\u001B[0m )\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# 9.3 Lookup Transformation - Add product_sk from product_df based on ProductID\u001B[39;00m\n\u001B[1;32m      9\u001B[0m merged_data_with_product_sk \u001B[38;5;241m=\u001B[39m merged_data\u001B[38;5;241m.\u001B[39mjoin(\n\u001B[0;32m---> 10\u001B[0m     product_df\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproduct_id\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m     11\u001B[0m     merged_data\u001B[38;5;241m.\u001B[39mProduct_id \u001B[38;5;241m==\u001B[39m product_df\u001B[38;5;241m.\u001B[39mproduct_id,\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mleft\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     13\u001B[0m )\u001B[38;5;241m.\u001B[39mdrop(product_df\u001B[38;5;241m.\u001B[39mproduct_id)\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# 9.4 DRC - Replace nulls\u001B[39;00m\n\u001B[1;32m     16\u001B[0m merged_data_validated \u001B[38;5;241m=\u001B[39m merged_data_with_product_sk \\\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTaxAmt\u001B[39m\u001B[38;5;124m\"\u001B[39m, coalesce(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTaxAmt\u001B[39m\u001B[38;5;124m\"\u001B[39m), lit(\u001B[38;5;241m0\u001B[39m))) \\\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFreight\u001B[39m\u001B[38;5;124m\"\u001B[39m, coalesce(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFreight\u001B[39m\u001B[38;5;124m\"\u001B[39m), lit(\u001B[38;5;241m0\u001B[39m))) \\\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproduct_sk\u001B[39m\u001B[38;5;124m\"\u001B[39m, coalesce(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproduct_sk\u001B[39m\u001B[38;5;124m\"\u001B[39m), lit(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)))\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3023\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m   2978\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   2979\u001B[0m     \u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m   2980\u001B[0m \n\u001B[1;32m   2981\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3021\u001B[0m \u001B[38;5;124;03m    +-----+---+\u001B[39;00m\n\u001B[1;32m   3022\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 3023\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jcols\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3024\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `product_id` cannot be resolved. Did you mean one of the following? [`ProductID`, `ProductLine`, `rowguid`, `ProductModelID`, `ProductNumber`].;\n'Project ['product_id]\n+- Relation [ProductID#5368,Name#5369,ProductNumber#5370,MakeFlag#5371,FinishedGoodsFlag#5372,Color#5373,SafetyStockLevel#5374,ReorderPoint#5375,StandardCost#5376,ListPrice#5377,Size#5378,SizeUnitMeasureCode#5379,WeightUnitMeasureCode#5380,Weight#5381,DaysToManufacture#5382,ProductLine#5383,Class#5384,Style#5385,ProductSubcategoryID#5386,ProductModelID#5387,SellStartDate#5388,SellEndDate#5389,DiscontinuedDate#5390,rowguid#5391,ModifiedDate#5392] csv\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `product_id` cannot be resolved. Did you mean one of the following? [`ProductID`, `ProductLine`, `rowguid`, `ProductModelID`, `ProductNumber`].;\n'Project ['product_id]\n+- Relation [ProductID#5368,Name#5369,ProductNumber#5370,MakeFlag#5371,FinishedGoodsFlag#5372,Color#5373,SafetyStockLevel#5374,ReorderPoint#5375,StandardCost#5376,ListPrice#5377,Size#5378,SizeUnitMeasureCode#5379,WeightUnitMeasureCode#5380,Weight#5381,DaysToManufacture#5382,ProductLine#5383,Class#5384,Style#5385,ProductSubcategoryID#5386,ProductModelID#5387,SellStartDate#5388,SellEndDate#5389,DiscontinuedDate#5390,rowguid#5391,ModifiedDate#5392] csv\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Merge join on SalesOrderID\n",
    "merged_data = sales_order_header_sorted.join(\n",
    "    sales_order_detail_sorted,\n",
    "    \"SalesOrderID\",\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "# 9.3 Lookup Transformation - Add product_sk from product_df based on ProductID\n",
    "merged_data_with_product_sk = merged_data.join(\n",
    "    product_df.select(\"product_id\"),\n",
    "    merged_data.Product_id == product_df.product_id,\n",
    "    \"left\"\n",
    ").drop(product_df.product_id)\n",
    "\n",
    "# 9.4 DRC - Replace nulls\n",
    "merged_data_validated = merged_data_with_product_sk \\\n",
    "    .withColumn(\"TaxAmt\", coalesce(col(\"TaxAmt\"), lit(0))) \\\n",
    "    .withColumn(\"Freight\", coalesce(col(\"Freight\"), lit(0))) \\\n",
    "    .withColumn(\"product_sk\", coalesce(col(\"product_sk\"), lit(-1)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c53afdb1-f1b1-4476-924e-35baa47441f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e888dd5-88f6-454e-8148-6b7e7339ee06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fe7d85f-ef15-4c34-be91-9f86eae924d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 9.6 Load Phase - Select columns for the fact table\n",
    "fact_table = merged_data_final.select(\n",
    "    col(\"product_sk\"),\n",
    "    col(\"SalesOrderID\").alias(\"sales_order_id\"),\n",
    "    col(\"SalesOrderDetailID\").alias(\"line_number\"),\n",
    "    col(\"OrderQty\").alias(\"quantity\"),\n",
    "    col(\"UnitPrice\").alias(\"unit_price\"),\n",
    "    col(\"standard_cost\").alias(\"unit_cost\"),\n",
    "    col(\"TaxAmt\").alias(\"tax_amount\"),\n",
    "    col(\"Freight\").alias(\"freight\"),\n",
    "    col(\"extended_sales\"),\n",
    "    col(\"OrderDate\").alias(\"created_at\"),\n",
    "    col(\"extended_cost\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "187c18de-e8e3-4ad8-ae7f-b936b3aac6fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------------------+-------------+-------------+\n|product_id|product_name|product_description|reorder_point|standard_cost|\n+----------+------------+-------------------+-------------+-------------+\n|         1|        null|               null|          750|          0.0|\n|         2|        null|               null|          750|          0.0|\n|         3|        null|               null|          600|          0.0|\n|         4|        null|               null|          600|          0.0|\n|       316|        null|               null|          600|          0.0|\n+----------+------------+-------------------+-------------+-------------+\nonly showing top 5 rows\n\n+------------+----------------+----------+----------+-----------+--------+-------+\n|SalesOrderID|SalesOrderNumber| OrderDate|CustomerID|TerritoryID|  TaxAmt|Freight|\n+------------+----------------+----------+----------+-----------+--------+-------+\n|       43697|         SO43697|2011-05-31|     21768|          6|286.2616|89.4568|\n|       43698|         SO43698|2011-05-31|     28389|          7|271.9992|84.9998|\n|       43699|         SO43699|2011-05-31|     25863|          1|271.9992|84.9998|\n|       43700|         SO43700|2011-05-31|     14501|          4| 55.9279|17.4775|\n|       43701|         SO43701|2011-05-31|     11003|          9|271.9992|84.9998|\n+------------+----------------+----------+----------+-----------+--------+-------+\nonly showing top 5 rows\n\n+------------+------------------+---------------------+--------+---------+--------------+---------+-----------------+---------+--------------------+-------------------+\n|SalesOrderID|SalesOrderDetailID|CarrierTrackingNumber|OrderQty|ProductID|SpecialOfferID|UnitPrice|UnitPriceDiscount|LineTotal|             rowguid|       ModifiedDate|\n+------------+------------------+---------------------+--------+---------+--------------+---------+-----------------+---------+--------------------+-------------------+\n|       43659|                 1|         4911-403C-98|       1|      776|             1| 2024.994|              0.0| 2024.994|B207C96D-D9E6-402...|2011-05-31 00:00:00|\n|       43659|                 2|         4911-403C-98|       3|      777|             1| 2024.994|              0.0| 6074.982|7ABB600D-1E77-41B...|2011-05-31 00:00:00|\n|       43659|                 3|         4911-403C-98|       1|      778|             1| 2024.994|              0.0| 2024.994|475CF8C6-49F6-486...|2011-05-31 00:00:00|\n|       43659|                 4|         4911-403C-98|       1|      771|             1| 2039.994|              0.0| 2039.994|04C4DE91-5815-45D...|2011-05-31 00:00:00|\n|       43659|                 5|         4911-403C-98|       1|      772|             1| 2039.994|              0.0| 2039.994|5A74C7D2-E641-438...|2011-05-31 00:00:00|\n+------------+------------------+---------------------+--------+---------+--------------+---------+-----------------+---------+--------------------+-------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "product_dim.show(5)\n",
    "sales_order_header.show(5)\n",
    "sales_order_detail.show(5)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Fact_dim_product_Table 2024-11-10 09:56:13",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
