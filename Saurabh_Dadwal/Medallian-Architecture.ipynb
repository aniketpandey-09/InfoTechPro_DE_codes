{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28be06ab-1917-40c7-92e6-a08d7273da56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Step 1\n",
    "Generate Synthetic Data for Bronze Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53efb54b-37b8-4e13-a854-e327fbbdc188",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DoubleType\n",
    "import datetime\n",
    "\n",
    "# Define schema for the sales data\n",
    "sales_schema = StructType([\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"sale_date\", TimestampType(), True),\n",
    "    StructField(\"sale_amount\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Generate synthetic sales data\n",
    "sales_data = [\n",
    "    (\"user1\", \"productA\", datetime.datetime(2024, 11, 5), 25.99),\n",
    "    (\"user2\", \"productB\", datetime.datetime(2024, 11, 5), 15.49),\n",
    "    (\"user3\", \"productC\", datetime.datetime(2024, 11, 5), 35.00),\n",
    "    (\"user4\", \"productA\", datetime.datetime(2024, 11, 5), 25.99),\n",
    "    (\"user1\", \"productD\", datetime.datetime(2024, 11, 6), 45.00),\n",
    "]\n",
    "\n",
    "# Create DataFrame with the correct schema\n",
    "sales_df = spark.createDataFrame(sales_data, schema=sales_schema)\n",
    "\n",
    "# Save this DataFrame as the sales table in the Bronze layer\n",
    "sales_df.write.mode(\"overwrite\").format(\"parquet\").saveAsTable(\"sales\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "228a7bcf-6e86-4c70-91ad-68cf45d026a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Clean and Transform Sales Data in the Silver Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e901a007-0da0-4664-9c1e-c5e5026b0788",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "# Load the sales data from Bronze layer\n",
    "bronze_sales_df = spark.table(\"sales\")\n",
    "\n",
    "# Clean the sales data\n",
    "silver_sales_df = (bronze_sales_df\n",
    "                   .dropDuplicates()\n",
    "                   .withColumn(\"sale_date\", to_timestamp(\"sale_date\")))  # Ensure sale_date is a timestamp\n",
    "\n",
    "# Save the cleaned sales data to the Silver layer\n",
    "silver_sales_df.write.mode(\"overwrite\").format(\"parquet\").saveAsTable(\"cleaned_sales\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47afb066-68d4-4ef8-99ac-94faaadf107e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " Aggregate Sales Data in the Gold Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfea5f86-f35e-4725-a403-4897c174cbe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Load the cleaned sales data from Silver layer\n",
    "silver_sales_df = spark.table(\"cleaned_sales\")\n",
    "\n",
    "# Calculate daily total sales\n",
    "daily_sales_df = (silver_sales_df\n",
    "                  .groupBy(\"sale_date\")\n",
    "                  .agg(sum(\"sale_amount\").alias(\"total_sales\")))\n",
    "\n",
    "# Save the aggregated sales data to the Gold layer\n",
    "daily_sales_df.write.mode(\"overwrite\").format(\"parquet\").saveAsTable(\"daily_sales\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Medallian-Architecture",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
