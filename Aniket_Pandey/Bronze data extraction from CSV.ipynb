{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b159928-c959-463a-afe5-9c4bf60a929b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "\n",
    "# Initialize SparkSession with the necessary option to handle S3\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AWS S3 Integration and Data Processing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set AWS S3 access keys securely\n",
    "# spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"AKIAZG4APFAQRRDBUO65\")\n",
    "# spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"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\")\n",
    "\n",
    "# Define S3 bucket and file paths\n",
    "# bucket_name = \"iiht-ntt-24\"\n",
    "# customer_file_path = f\"s3a://{bucket_name}/data.csv/Customer.csv\"\n",
    "customer_file_path = \"dbfs:/FileStore/tables/customer_data.csv\"\n",
    "\n",
    "# Define the schema for the Customer CSV based on the initial definition provided\n",
    "customer_schema = StructType([\n",
    "    StructField(\"CustomerID\", IntegerType(), True),\n",
    "    StructField(\"CustomerName\", StringType(), True),\n",
    "    StructField(\"BillToCustomerID\", StringType(), True),\n",
    "    StructField(\"CustomerCategoryID\", StringType(), True),\n",
    "    StructField(\"BuyingGroupID\", StringType(), True),\n",
    "    StructField(\"PrimaryContactPersonID\", StringType(), True),\n",
    "    StructField(\"PostalCityID\", StringType(), True),\n",
    "    StructField(\"ValidFrom\", DateType(), True),\n",
    "    StructField(\"ValidTo\", DateType(), True),\n",
    "    StructField(\"LineageKey\", IntegerType(), True)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b605410-b2bf-4c52-b576-f671d6d967cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_df = spark.read.format(\"csv\").schema(customer_schema).load(customer_file_path)\n",
    "\n",
    "# Transform the DataFrame as required\n",
    "customer_df_transformed = customer_df.selectExpr(\n",
    "    \"CustomerID as CustomerKey\",\n",
    "    \"CustomerID as WWICustomerID\",\n",
    "    \"CustomerName as Customer\",\n",
    "    \"BillToCustomerID as BillToCustomer\",\n",
    "    \"CustomerCategoryID as Category\",\n",
    "    \"BuyingGroupID as BuyingGroup\",\n",
    "    \"PrimaryContactPersonID as PrimaryContact\",\n",
    "    \"PostalCityID as PostalCode\",\n",
    "    \"ValidFrom as ValidFrom\",\n",
    "    \"ValidTo as ValidTo\",\n",
    "    \"LineageKey as LineageKey\"\n",
    ").withColumn(\"LineageKey\", lit(9))  # Set the value of LineageKey to 9\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ed83de9-dfec-4ab5-93d3-2b85d4be567d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table SilverCustomer created/overwritten successfully.\n"
     ]
    }
   ],
   "source": [
    "# Handle the SilverCustomer table creation or replacement with error handling\n",
    "table_name = \"SilverCustomer\"\n",
    "try:\n",
    "    if spark._jsparkSession.catalog().tableExists(table_name):\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "    customer_df_transformed.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "    print(f\"Table {table_name} created/overwritten successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to create or replace table {table_name} due to: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77e19c05-9bc2-4e68-942a-0017e5dc6b9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze data extraction from CSV",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
